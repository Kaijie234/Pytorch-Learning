{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 第一课"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 什么是PyTorch?\n",
    "\n",
    "model = Architecture + Parameters\n",
    "PyTorch是一个基于Python的科学计算库，它有以下特点:\n",
    "\n",
    "- 类似于NumPy，但是它可以使用GPU\n",
    "- 可以用它定义深度学习模型，可以灵活地进行深度学习模型的训练和使用\n",
    "\n",
    "## Tensors\n",
    "\n",
    "Tensor类似与NumPy的ndarray，唯一的区别是Tensor可以在GPU上加速运算。\n",
    "\n",
    "# 如何成为PyTorch大神？\n",
    "\n",
    "- 学好深度学习的基础知识\n",
    "- 学习PyTorch官方tutorial\n",
    "- 学习GitHub以及各种博客上的教程(别人创建好的list)\n",
    "- 阅读documentation，使用论坛https://discuss.pytorch.org/\n",
    "- 跑通以及学习开源PyTorch项目\n",
    "- 阅读深度学习模型paper，学习别人的模型实现\n",
    "- 通过阅读paper，自己实现模型\n",
    "- 自己创造模型(也可以写paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造一个未初始化的5x3矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.1837e-39, 4.6837e-39, 9.2755e-39],\n",
       "        [1.0837e-38, 8.4490e-39, 1.1112e-38],\n",
       "        [1.0194e-38, 9.0919e-39, 8.4490e-39],\n",
       "        [9.6429e-39, 8.4490e-39, 9.6429e-39],\n",
       "        [9.2755e-39, 1.0286e-38, 9.0919e-39]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.empty(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个随机初始化的矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2449, 0.8412, 0.6487],\n",
       "        [0.1779, 0.5634, 0.4302],\n",
       "        [0.5804, 0.2735, 0.9928],\n",
       "        [0.8712, 0.2792, 0.2858],\n",
       "        [0.8749, 0.4141, 0.8015]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建一个全部为0，类型为long的矩阵:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5,3,dtype=torch.long)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(5,3).long()\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从数据直接直接构建tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.5000, 3.0000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([5.5,3])\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以从一个已有的tensor构建一个tensor。这些方法会重用原来tensor的特征，例如，数据类型，除非提供新的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.new_ones(5,3, dtype=torch.double)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.5098e-01, -2.1833e+00,  1.7353e+00],\n",
       "        [-1.9543e+00,  1.3971e-03,  9.1297e-02],\n",
       "        [ 2.4668e+00,  1.3432e+00,  1.4745e-01],\n",
       "        [-1.1110e+00, -8.8386e-01,  1.5700e+00],\n",
       "        [ 5.7871e-01,  6.8120e-01, -1.5284e-01]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn_like(x, dtype=torch.float)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到tensor的形状:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>注意</h4><p>``torch.Size`` 返回的是一个tuple</p></div>\n",
    "\n",
    "Operations\n",
    "\n",
    "\n",
    "有很多种tensor运算。我们先介绍加法运算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2833, 0.6793, 0.4296],\n",
       "        [0.6732, 0.1925, 0.0823],\n",
       "        [0.1578, 0.9667, 0.8093],\n",
       "        [0.4643, 0.9432, 0.9148],\n",
       "        [0.6818, 0.3293, 0.4672]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand(5,3)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0343, -1.5040,  2.1649],\n",
       "        [-1.2811,  0.1939,  0.1736],\n",
       "        [ 2.6247,  2.3099,  0.9568],\n",
       "        [-0.6467,  0.0593,  2.4848],\n",
       "        [ 1.2605,  1.0105,  0.3143]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种着加法的写法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0343, -1.5040,  2.1649],\n",
       "        [-1.2811,  0.1939,  0.1736],\n",
       "        [ 2.6247,  2.3099,  0.9568],\n",
       "        [-0.6467,  0.0593,  2.4848],\n",
       "        [ 1.2605,  1.0105,  0.3143]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.add(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加法：把输出作为一个变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0343, -1.5040,  2.1649],\n",
       "        [-1.2811,  0.1939,  0.1736],\n",
       "        [ 2.6247,  2.3099,  0.9568],\n",
       "        [-0.6467,  0.0593,  2.4848],\n",
       "        [ 1.2605,  1.0105,  0.3143]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = torch.empty(5,3)\n",
    "torch.add(x, y, out=result)\n",
    "# result = x + y\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in-place加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0343, -1.5040,  2.1649],\n",
       "        [-1.2811,  0.1939,  0.1736],\n",
       "        [ 2.6247,  2.3099,  0.9568],\n",
       "        [-0.6467,  0.0593,  2.4848],\n",
       "        [ 1.2605,  1.0105,  0.3143]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>注意</h4><p>任何in-place的运算都会以``_``结尾。\n",
    "    举例来说：``x.copy_(y)``, ``x.t_()``, 会改变 ``x``。</p></div>\n",
    "\n",
    "各种类似NumPy的indexing都可以在PyTorch tensor上面使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.3971e-03,  9.1297e-02],\n",
       "        [ 1.3432e+00,  1.4745e-01],\n",
       "        [-8.8386e-01,  1.5700e+00],\n",
       "        [ 6.8120e-01, -1.5284e-01]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[1:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resizing: 如果你希望resize/reshape一个tensor，可以使用``torch.view``："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3771,  0.1132, -0.3402, -1.3023,  0.8325,  0.1971, -0.6270,  0.5392],\n",
       "        [-1.9797, -1.3136, -1.2431,  1.5669,  0.7865,  0.9015, -0.6884, -1.0703]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(4,4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1,8)\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你有一个只有一个元素的tensor，使用``.item()``方法可以把里面的value变成Python数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.6816])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.6816351413726807"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.3771, -1.9797],\n",
       "        [ 0.1132, -1.3136],\n",
       "        [-0.3402, -1.2431],\n",
       "        [-1.3023,  1.5669],\n",
       "        [ 0.8325,  0.7865],\n",
       "        [ 0.1971,  0.9015],\n",
       "        [-0.6270, -0.6884],\n",
       "        [ 0.5392, -1.0703]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.transpose(1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**更多阅读**\n",
    "\n",
    "\n",
    "  各种Tensor operations, 包括transposing, indexing, slicing,\n",
    "  mathematical operations, linear algebra, random numbers在\n",
    "  `<https://pytorch.org/docs/torch>`.\n",
    "\n",
    "Numpy和Tensor之间的转化\n",
    "------------\n",
    "\n",
    "在Torch Tensor和NumPy array之间相互转化非常容易。\n",
    "\n",
    "Torch Tensor和NumPy array会共享内存，所以改变其中一项也会改变另一项。\n",
    "\n",
    "把Torch Tensor转变成NumPy Array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改变numpy array里面的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1] = 2\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 1., 1., 1.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把NumPy ndarray转成Torch Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有CPU上的Tensor都支持转成numpy或者从numpy转成Tensor。\n",
    "\n",
    "CUDA Tensors\n",
    "------------\n",
    "\n",
    "使用``.to``方法，Tensor可以被移动到别的device上。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.6816], device='cuda:0')\n",
      "tensor([3.6816], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    y = torch.ones_like(x, device=device)\n",
    "    x = x.to(device)\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.to(\"cpu\").data.numpy()\n",
    "y.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-ba8fb779f1a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model = model.cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "热身: 用numpy实现两层神经网络\n",
    "--------------\n",
    "\n",
    "一个全连接ReLU神经网络，一个隐藏层，没有bias。用来从x预测y，使用L2 Loss。\n",
    "- $h = W_1X$\n",
    "- $a = max(0, h)$\n",
    "- $y_{hat} = W_2a$\n",
    "\n",
    "这一实现完全使用numpy来计算前向神经网络，loss，和反向传播。\n",
    "- forward pass\n",
    "- loss\n",
    "- backward pass\n",
    "\n",
    "numpy ndarray是一个普通的n维array。它不知道任何关于深度学习或者梯度(gradient)的知识，也不知道计算图(computation graph)，只是一种用来计算数学运算的数据结构。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    h = x.dot(w1) # N * H\n",
    "    h_relu = np.maximum(h, 0) # N * H\n",
    "    y_pred = h_relu.dot(w2) # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensors\n",
    "----------------\n",
    "\n",
    "这次我们使用PyTorch tensors来创建前向神经网络，计算损失，以及反向传播。\n",
    "\n",
    "一个PyTorch Tensor很像一个numpy的ndarray。但是它和numpy ndarray最大的区别是，PyTorch Tensor可以在CPU或者GPU上运算。如果想要在GPU上运算，就需要把Tensor换成cuda类型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H)\n",
    "w2 = torch.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    h = x.mm(w1) # N * H\n",
    "    h_relu = h.clamp(min=0) # N * H\n",
    "    y_pred = h_relu.mm(w2) # N * D_out\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    print(it, loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    # compute the gradient\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h<0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad=True)\n",
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(3., requires_grad=True)\n",
    "\n",
    "y = w*x + b # y = 2*1+3\n",
    "\n",
    "y.backward()\n",
    "\n",
    "# dy / dw = x\n",
    "print(w.grad)\n",
    "print(x.grad)\n",
    "print(b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: Tensor和autograd\n",
    "-------------------------------\n",
    "\n",
    "PyTorch的一个重要功能就是autograd，也就是说只要定义了forward pass(前向神经网络)，计算了loss之后，PyTorch可以自动求导计算模型所有参数的梯度。\n",
    "\n",
    "一个PyTorch的Tensor表示计算图中的一个节点。如果``x``是一个Tensor并且``x.requires_grad=True``那么``x.grad``是另一个储存着``x``当前梯度(相对于一个scalar，常常是loss)的向量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "w1 = torch.randn(D_in, H, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = (y_pred - y).pow(2).sum() # computation graph\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: nn\n",
    "-----------\n",
    "\n",
    "\n",
    "这次我们使用PyTorch中nn这个库来构建网络。\n",
    "用PyTorch autograd来构建计算图和计算gradients，\n",
    "然后PyTorch会帮我们自动计算gradient。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30796758.0\n",
      "1 31658336.0\n",
      "2 35340732.0\n",
      "3 35842768.0\n",
      "4 29652120.0\n",
      "5 19048664.0\n",
      "6 9998813.0\n",
      "7 4842135.0\n",
      "8 2503577.75\n",
      "9 1502023.125\n",
      "10 1045942.25\n",
      "11 805809.4375\n",
      "12 656833.125\n",
      "13 551465.1875\n",
      "14 470619.78125\n",
      "15 405676.625\n",
      "16 352232.21875\n",
      "17 307525.8125\n",
      "18 269770.96875\n",
      "19 237652.015625\n",
      "20 210176.09375\n",
      "21 186538.578125\n",
      "22 166096.75\n",
      "23 148363.03125\n",
      "24 132908.125\n",
      "25 119409.46875\n",
      "26 107558.53125\n",
      "27 97133.3671875\n",
      "28 87936.59375\n",
      "29 79787.796875\n",
      "30 72550.4375\n",
      "31 66099.953125\n",
      "32 60338.90234375\n",
      "33 55180.42578125\n",
      "34 50549.44140625\n",
      "35 46382.1953125\n",
      "36 42627.51171875\n",
      "37 39237.1875\n",
      "38 36168.2890625\n",
      "39 33386.96875\n",
      "40 30861.333984375\n",
      "41 28562.126953125\n",
      "42 26468.365234375\n",
      "43 24556.771484375\n",
      "44 22809.75\n",
      "45 21210.68359375\n",
      "46 19745.548828125\n",
      "47 18402.154296875\n",
      "48 17167.94921875\n",
      "49 16032.509765625\n",
      "50 14986.251953125\n",
      "51 14020.9482421875\n",
      "52 13129.1328125\n",
      "53 12303.9296875\n",
      "54 11539.8408203125\n",
      "55 10831.37890625\n",
      "56 10174.25390625\n",
      "57 9564.1943359375\n",
      "58 8996.5107421875\n",
      "59 8468.296875\n",
      "60 7976.27783203125\n",
      "61 7517.33251953125\n",
      "62 7089.07177734375\n",
      "63 6688.98876953125\n",
      "64 6314.91455078125\n",
      "65 5965.03173828125\n",
      "66 5637.306640625\n",
      "67 5330.10888671875\n",
      "68 5042.11865234375\n",
      "69 4771.9658203125\n",
      "70 4518.37939453125\n",
      "71 4280.1796875\n",
      "72 4056.259033203125\n",
      "73 3845.92236328125\n",
      "74 3647.99658203125\n",
      "75 3461.456787109375\n",
      "76 3285.672607421875\n",
      "77 3119.98876953125\n",
      "78 2963.5693359375\n",
      "79 2815.8369140625\n",
      "80 2676.335205078125\n",
      "81 2544.458984375\n",
      "82 2419.77783203125\n",
      "83 2301.844482421875\n",
      "84 2190.27490234375\n",
      "85 2084.656005859375\n",
      "86 1984.619140625\n",
      "87 1889.8243408203125\n",
      "88 1800.0234375\n",
      "89 1714.844970703125\n",
      "90 1634.0054931640625\n",
      "91 1557.3179931640625\n",
      "92 1484.5330810546875\n",
      "93 1415.5284423828125\n",
      "94 1350.0093994140625\n",
      "95 1287.7730712890625\n",
      "96 1228.661865234375\n",
      "97 1172.484619140625\n",
      "98 1119.05615234375\n",
      "99 1068.2578125\n",
      "100 1019.9268188476562\n",
      "101 973.9608764648438\n",
      "102 930.2057495117188\n",
      "103 888.549560546875\n",
      "104 848.9005126953125\n",
      "105 811.1409301757812\n",
      "106 775.169677734375\n",
      "107 740.9105224609375\n",
      "108 708.253173828125\n",
      "109 677.124755859375\n",
      "110 647.4596557617188\n",
      "111 619.1776123046875\n",
      "112 592.2439575195312\n",
      "113 566.56298828125\n",
      "114 542.1146240234375\n",
      "115 518.7922973632812\n",
      "116 496.52569580078125\n",
      "117 475.27044677734375\n",
      "118 454.97723388671875\n",
      "119 435.5940246582031\n",
      "120 417.07940673828125\n",
      "121 399.3955383300781\n",
      "122 382.4959716796875\n",
      "123 366.3557434082031\n",
      "124 350.93353271484375\n",
      "125 336.199951171875\n",
      "126 322.11077880859375\n",
      "127 308.63409423828125\n",
      "128 295.7496643066406\n",
      "129 283.42864990234375\n",
      "130 271.64215087890625\n",
      "131 260.3685607910156\n",
      "132 249.58578491210938\n",
      "133 239.276123046875\n",
      "134 229.41954040527344\n",
      "135 219.98736572265625\n",
      "136 210.95721435546875\n",
      "137 202.31488037109375\n",
      "138 194.03916931152344\n",
      "139 186.1302032470703\n",
      "140 178.5594024658203\n",
      "141 171.30953979492188\n",
      "142 164.36721801757812\n",
      "143 157.7136688232422\n",
      "144 151.34072875976562\n",
      "145 145.23580932617188\n",
      "146 139.38465881347656\n",
      "147 133.77857971191406\n",
      "148 128.40560913085938\n",
      "149 123.25768280029297\n",
      "150 118.32218933105469\n",
      "151 113.59204864501953\n",
      "152 109.0575942993164\n",
      "153 104.70878601074219\n",
      "154 100.54003143310547\n",
      "155 96.54413604736328\n",
      "156 92.71121215820312\n",
      "157 89.03649139404297\n",
      "158 85.51000213623047\n",
      "159 82.1299819946289\n",
      "160 78.88799285888672\n",
      "161 75.7753677368164\n",
      "162 72.79167175292969\n",
      "163 69.92765808105469\n",
      "164 67.18026733398438\n",
      "165 64.54462432861328\n",
      "166 62.01408767700195\n",
      "167 59.58662796020508\n",
      "168 57.25666046142578\n",
      "169 55.02060317993164\n",
      "170 52.874481201171875\n",
      "171 50.81498718261719\n",
      "172 48.83658218383789\n",
      "173 46.937774658203125\n",
      "174 45.11495590209961\n",
      "175 43.36552047729492\n",
      "176 41.68518829345703\n",
      "177 40.07232666015625\n",
      "178 38.5231819152832\n",
      "179 37.03535079956055\n",
      "180 35.60707092285156\n",
      "181 34.23538589477539\n",
      "182 32.91741943359375\n",
      "183 31.651769638061523\n",
      "184 30.436059951782227\n",
      "185 29.268165588378906\n",
      "186 28.14596939086914\n",
      "187 27.068302154541016\n",
      "188 26.032238006591797\n",
      "189 25.03767204284668\n",
      "190 24.0810489654541\n",
      "191 23.16276741027832\n",
      "192 22.28033447265625\n",
      "193 21.43214225769043\n",
      "194 20.617341995239258\n",
      "195 19.83322525024414\n",
      "196 19.080936431884766\n",
      "197 18.357362747192383\n",
      "198 17.661853790283203\n",
      "199 16.992902755737305\n",
      "200 16.350814819335938\n",
      "201 15.733154296875\n",
      "202 15.138873100280762\n",
      "203 14.567648887634277\n",
      "204 14.018991470336914\n",
      "205 13.491057395935059\n",
      "206 12.983914375305176\n",
      "207 12.49606704711914\n",
      "208 12.026676177978516\n",
      "209 11.575377464294434\n",
      "210 11.141317367553711\n",
      "211 10.724251747131348\n",
      "212 10.322894096374512\n",
      "213 9.936797142028809\n",
      "214 9.56558895111084\n",
      "215 9.209176063537598\n",
      "216 8.865089416503906\n",
      "217 8.535001754760742\n",
      "218 8.217278480529785\n",
      "219 7.911279678344727\n",
      "220 7.617275238037109\n",
      "221 7.334380626678467\n",
      "222 7.062033176422119\n",
      "223 6.800240516662598\n",
      "224 6.548359394073486\n",
      "225 6.305839538574219\n",
      "226 6.0725579261779785\n",
      "227 5.848154544830322\n",
      "228 5.632070064544678\n",
      "229 5.424233913421631\n",
      "230 5.224125385284424\n",
      "231 5.031762599945068\n",
      "232 4.84645938873291\n",
      "233 4.668144226074219\n",
      "234 4.496237277984619\n",
      "235 4.331085681915283\n",
      "236 4.172178268432617\n",
      "237 4.019150257110596\n",
      "238 3.8716979026794434\n",
      "239 3.729924201965332\n",
      "240 3.59334135055542\n",
      "241 3.461866617202759\n",
      "242 3.3353006839752197\n",
      "243 3.21337890625\n",
      "244 3.096107006072998\n",
      "245 2.9832205772399902\n",
      "246 2.8744497299194336\n",
      "247 2.7698795795440674\n",
      "248 2.668973445892334\n",
      "249 2.5719709396362305\n",
      "250 2.478396415710449\n",
      "251 2.3884036540985107\n",
      "252 2.30168080329895\n",
      "253 2.2181520462036133\n",
      "254 2.1377015113830566\n",
      "255 2.0602145195007324\n",
      "256 1.9856210947036743\n",
      "257 1.9138447046279907\n",
      "258 1.844651699066162\n",
      "259 1.7779911756515503\n",
      "260 1.7137750387191772\n",
      "261 1.6519185304641724\n",
      "262 1.5923712253570557\n",
      "263 1.5351225137710571\n",
      "264 1.4797853231430054\n",
      "265 1.4264976978302002\n",
      "266 1.3752442598342896\n",
      "267 1.3257770538330078\n",
      "268 1.2781076431274414\n",
      "269 1.2322810888290405\n",
      "270 1.1878912448883057\n",
      "271 1.1453570127487183\n",
      "272 1.1043691635131836\n",
      "273 1.0648705959320068\n",
      "274 1.026708960533142\n",
      "275 0.9900395274162292\n",
      "276 0.9545577764511108\n",
      "277 0.9204621911048889\n",
      "278 0.8876416683197021\n",
      "279 0.8558799028396606\n",
      "280 0.8254299163818359\n",
      "281 0.7959825992584229\n",
      "282 0.7675929069519043\n",
      "283 0.7403222322463989\n",
      "284 0.7139432430267334\n",
      "285 0.6885618567466736\n",
      "286 0.6640864014625549\n",
      "287 0.6405491828918457\n",
      "288 0.6177826523780823\n",
      "289 0.5958333611488342\n",
      "290 0.5747197270393372\n",
      "291 0.5543696284294128\n",
      "292 0.5346851944923401\n",
      "293 0.5158523321151733\n",
      "294 0.4975372850894928\n",
      "295 0.47999307513237\n",
      "296 0.4630541503429413\n",
      "297 0.4466870427131653\n",
      "298 0.43093952536582947\n",
      "299 0.4156915843486786\n",
      "300 0.4010762572288513\n",
      "301 0.3869439661502838\n",
      "302 0.37327373027801514\n",
      "303 0.36008358001708984\n",
      "304 0.3474200367927551\n",
      "305 0.3351958394050598\n",
      "306 0.32338255643844604\n",
      "307 0.3120541572570801\n",
      "308 0.3010694682598114\n",
      "309 0.29051142930984497\n",
      "310 0.2803017199039459\n",
      "311 0.27048805356025696\n",
      "312 0.2610257565975189\n",
      "313 0.2518713176250458\n",
      "314 0.24305617809295654\n",
      "315 0.2345484346151352\n",
      "316 0.22633594274520874\n",
      "317 0.21842290461063385\n",
      "318 0.21076396107673645\n",
      "319 0.2034672349691391\n",
      "320 0.19632801413536072\n",
      "321 0.1895056664943695\n",
      "322 0.18286308646202087\n",
      "323 0.17648108303546906\n",
      "324 0.17037604749202728\n",
      "325 0.1644020527601242\n",
      "326 0.15870818495750427\n",
      "327 0.1531611680984497\n",
      "328 0.14782781898975372\n",
      "329 0.1427067369222641\n",
      "330 0.13771957159042358\n",
      "331 0.13295967876911163\n",
      "332 0.12832598388195038\n",
      "333 0.12387101352214813\n",
      "334 0.1195850670337677\n",
      "335 0.11541957408189774\n",
      "336 0.11140602082014084\n",
      "337 0.10755041241645813\n",
      "338 0.10381845384836197\n",
      "339 0.1002388447523117\n",
      "340 0.09677790850400925\n",
      "341 0.09345340728759766\n",
      "342 0.09023752063512802\n",
      "343 0.08709409087896347\n",
      "344 0.08408807218074799\n",
      "345 0.08121950179338455\n",
      "346 0.07840155810117722\n",
      "347 0.07570235431194305\n",
      "348 0.07307863235473633\n",
      "349 0.07057733088731766\n",
      "350 0.06812529265880585\n",
      "351 0.06580346077680588\n",
      "352 0.06354764103889465\n",
      "353 0.06136123463511467\n",
      "354 0.059257086366415024\n",
      "355 0.05721699446439743\n",
      "356 0.05525650456547737\n",
      "357 0.05337408930063248\n",
      "358 0.051545049995183945\n",
      "359 0.049766384065151215\n",
      "360 0.048073552548885345\n",
      "361 0.04643261060118675\n",
      "362 0.04485198110342026\n",
      "363 0.043302588164806366\n",
      "364 0.041818227618932724\n",
      "365 0.04039113596081734\n",
      "366 0.03901698440313339\n",
      "367 0.037693560123443604\n",
      "368 0.036404382437467575\n",
      "369 0.035156141966581345\n",
      "370 0.03397300839424133\n",
      "371 0.03282376751303673\n",
      "372 0.03169738128781319\n",
      "373 0.03063490055501461\n",
      "374 0.02958541177213192\n",
      "375 0.02858402021229267\n",
      "376 0.027618037536740303\n",
      "377 0.026679815724492073\n",
      "378 0.025782596319913864\n",
      "379 0.024910403415560722\n",
      "380 0.024069353938102722\n",
      "381 0.02326795645058155\n",
      "382 0.022490764036774635\n",
      "383 0.021733222529292107\n",
      "384 0.02100858837366104\n",
      "385 0.02029489167034626\n",
      "386 0.019612977281212807\n",
      "387 0.018958136439323425\n",
      "388 0.018321111798286438\n",
      "389 0.017715871334075928\n",
      "390 0.017122333869338036\n",
      "391 0.016555216163396835\n",
      "392 0.016006892547011375\n",
      "393 0.015467176213860512\n",
      "394 0.014954742044210434\n",
      "395 0.01446634903550148\n",
      "396 0.013981335796415806\n",
      "397 0.013515157625079155\n",
      "398 0.01306917518377304\n",
      "399 0.012634295970201492\n",
      "400 0.012220981530845165\n",
      "401 0.011824065819382668\n",
      "402 0.011440526694059372\n",
      "403 0.011064846999943256\n",
      "404 0.01069573126733303\n",
      "405 0.010349842719733715\n",
      "406 0.010007648728787899\n",
      "407 0.009686252102255821\n",
      "408 0.0093733761459589\n",
      "409 0.009066645987331867\n",
      "410 0.0087753189727664\n",
      "411 0.00849282369017601\n",
      "412 0.008216329850256443\n",
      "413 0.007954427972435951\n",
      "414 0.007699510082602501\n",
      "415 0.007451050914824009\n",
      "416 0.007212579250335693\n",
      "417 0.006981411948800087\n",
      "418 0.006763383746147156\n",
      "419 0.00654419232159853\n",
      "420 0.006341070402413607\n",
      "421 0.006141159683465958\n",
      "422 0.0059490199200809\n",
      "423 0.005761489272117615\n",
      "424 0.0055778673849999905\n",
      "425 0.00540938088670373\n",
      "426 0.005240381229668856\n",
      "427 0.00507474597543478\n",
      "428 0.0049169715493917465\n",
      "429 0.00476487260311842\n",
      "430 0.004617302678525448\n",
      "431 0.0044791558757424355\n",
      "432 0.004339598584920168\n",
      "433 0.004208456724882126\n",
      "434 0.004079701378941536\n",
      "435 0.003959648311138153\n",
      "436 0.0038359949830919504\n",
      "437 0.0037275454960763454\n",
      "438 0.0036146221682429314\n",
      "439 0.003510010428726673\n",
      "440 0.0034043029882013798\n",
      "441 0.0033037562388926744\n",
      "442 0.003203044179826975\n",
      "443 0.0031104013323783875\n",
      "444 0.003015046240761876\n",
      "445 0.002929112408310175\n",
      "446 0.002846012357622385\n",
      "447 0.0027666313108056784\n",
      "448 0.0026862036902457476\n",
      "449 0.0026086787693202496\n",
      "450 0.002533655147999525\n",
      "451 0.002462217351421714\n",
      "452 0.0023933767806738615\n",
      "453 0.002324167639017105\n",
      "454 0.002260860288515687\n",
      "455 0.0021969920489937067\n",
      "456 0.002138313837349415\n",
      "457 0.0020766835659742355\n",
      "458 0.002019798383116722\n",
      "459 0.001966196810826659\n",
      "460 0.0019113867310807109\n",
      "461 0.0018607804086059332\n",
      "462 0.0018101754831150174\n",
      "463 0.0017619157442823052\n",
      "464 0.0017140958225354552\n",
      "465 0.0016687923343852162\n",
      "466 0.0016244737198576331\n",
      "467 0.0015828170580789447\n",
      "468 0.0015414997469633818\n",
      "469 0.0015023044543340802\n",
      "470 0.0014641886809840798\n",
      "471 0.0014245621860027313\n",
      "472 0.0013885133666917682\n",
      "473 0.0013530395226553082\n",
      "474 0.0013189987512305379\n",
      "475 0.0012855676468461752\n",
      "476 0.001253472059033811\n",
      "477 0.001223033177666366\n",
      "478 0.0011934356298297644\n",
      "479 0.0011644158512353897\n",
      "480 0.0011362056247889996\n",
      "481 0.0011103488504886627\n",
      "482 0.0010806224308907986\n",
      "483 0.0010553945321589708\n",
      "484 0.0010295885149389505\n",
      "485 0.0010055191814899445\n",
      "486 0.0009819255210459232\n",
      "487 0.0009583573555573821\n",
      "488 0.0009356132941320539\n",
      "489 0.0009130567195825279\n",
      "490 0.0008926148293539882\n",
      "491 0.0008722571074031293\n",
      "492 0.000850782438647002\n",
      "493 0.000831835437566042\n",
      "494 0.0008130577625706792\n",
      "495 0.0007938470225781202\n",
      "496 0.0007748512434773147\n",
      "497 0.0007597859366796911\n",
      "498 0.0007420512265525758\n",
      "499 0.0007252442883327603\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False), # w_1 * x + b_1\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    print(it, loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights of w1 and w2\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters(): # param (tensor, grad)\n",
    "            param -= learning_rate * param.grad\n",
    "            \n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-2.4369, -1.2259,  1.3928,  ..., -0.6311,  1.0741, -0.0186],\n",
       "        [-0.4728, -0.8807, -0.3484,  ...,  0.4950, -0.4096,  0.7196],\n",
       "        [-0.0301, -1.0491, -1.3055,  ...,  0.3130,  0.5399,  0.6213],\n",
       "        ...,\n",
       "        [-0.6895,  0.0346, -0.2390,  ..., -0.6945,  0.6638, -1.1315],\n",
       "        [ 0.8071,  1.0557,  0.1674,  ..., -0.9313,  0.0808, -1.0955],\n",
       "        [ 0.5790, -0.4493,  0.8469,  ...,  0.2945, -1.5833, -0.7447]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: optim\n",
    "--------------\n",
    "\n",
    "这一次我们不再手动更新模型的weights,而是使用optim这个包来帮助我们更新参数。\n",
    "optim这个package提供了各种不同的模型优化方法，包括SGD+momentum, RMSProp, Adam等等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 31203018.0\n",
      "1 24459852.0\n",
      "2 19565672.0\n",
      "3 14788742.0\n",
      "4 10435055.0\n",
      "5 6971237.5\n",
      "6 4576112.0\n",
      "7 3042284.0\n",
      "8 2102442.25\n",
      "9 1523112.0\n",
      "10 1156355.25\n",
      "11 913097.125\n",
      "12 743221.625\n",
      "13 618317.125\n",
      "14 522565.875\n",
      "15 446830.75\n",
      "16 385507.25\n",
      "17 335017.375\n",
      "18 292761.3125\n",
      "19 257028.1875\n",
      "20 226631.328125\n",
      "21 200571.5\n",
      "22 178084.078125\n",
      "23 158592.921875\n",
      "24 141631.09375\n",
      "25 126793.453125\n",
      "26 113775.9140625\n",
      "27 102356.125\n",
      "28 92280.3203125\n",
      "29 83363.0390625\n",
      "30 75442.578125\n",
      "31 68389.53125\n",
      "32 62099.8203125\n",
      "33 56478.3203125\n",
      "34 51445.4921875\n",
      "35 46925.22265625\n",
      "36 42857.80078125\n",
      "37 39196.75\n",
      "38 35891.359375\n",
      "39 32906.9296875\n",
      "40 30201.1953125\n",
      "41 27745.16015625\n",
      "42 25513.787109375\n",
      "43 23481.83203125\n",
      "44 21630.525390625\n",
      "45 19940.89453125\n",
      "46 18402.841796875\n",
      "47 16997.359375\n",
      "48 15710.19921875\n",
      "49 14530.1640625\n",
      "50 13448.8984375\n",
      "51 12456.611328125\n",
      "52 11545.50390625\n",
      "53 10707.75\n",
      "54 9936.71484375\n",
      "55 9226.4912109375\n",
      "56 8572.30078125\n",
      "57 7968.474609375\n",
      "58 7410.970703125\n",
      "59 6895.8779296875\n",
      "60 6420.083984375\n",
      "61 5980.220703125\n",
      "62 5573.0625\n",
      "63 5195.95703125\n",
      "64 4846.794921875\n",
      "65 4523.04833984375\n",
      "66 4222.4794921875\n",
      "67 3943.646728515625\n",
      "68 3684.85498046875\n",
      "69 3444.395751953125\n",
      "70 3220.828125\n",
      "71 3012.975341796875\n",
      "72 2819.60107421875\n",
      "73 2639.51708984375\n",
      "74 2471.91650390625\n",
      "75 2315.7822265625\n",
      "76 2170.254150390625\n",
      "77 2034.4891357421875\n",
      "78 1907.92578125\n",
      "79 1789.7900390625\n",
      "80 1679.465087890625\n",
      "81 1576.4285888671875\n",
      "82 1480.2178955078125\n",
      "83 1390.3017578125\n",
      "84 1306.2186279296875\n",
      "85 1227.593994140625\n",
      "86 1154.0252685546875\n",
      "87 1085.14990234375\n",
      "88 1020.6717529296875\n",
      "89 960.3031616210938\n",
      "90 903.7448120117188\n",
      "91 850.7644653320312\n",
      "92 801.0674438476562\n",
      "93 754.4710693359375\n",
      "94 710.751953125\n",
      "95 669.7276611328125\n",
      "96 631.2488403320312\n",
      "97 595.1109008789062\n",
      "98 561.167236328125\n",
      "99 529.307373046875\n",
      "100 499.3644714355469\n",
      "101 471.2109680175781\n",
      "102 444.7438049316406\n",
      "103 419.8548278808594\n",
      "104 396.43157958984375\n",
      "105 374.4007568359375\n",
      "106 353.67529296875\n",
      "107 334.1713562011719\n",
      "108 315.7948303222656\n",
      "109 298.4830322265625\n",
      "110 282.183349609375\n",
      "111 266.8258361816406\n",
      "112 252.35362243652344\n",
      "113 238.70733642578125\n",
      "114 225.83642578125\n",
      "115 213.70802307128906\n",
      "116 202.26748657226562\n",
      "117 191.4678497314453\n",
      "118 181.27410888671875\n",
      "119 171.6566619873047\n",
      "120 162.5762176513672\n",
      "121 153.99908447265625\n",
      "122 145.90533447265625\n",
      "123 138.25613403320312\n",
      "124 131.0246124267578\n",
      "125 124.1971206665039\n",
      "126 117.73975372314453\n",
      "127 111.63457489013672\n",
      "128 105.86498260498047\n",
      "129 100.40745544433594\n",
      "130 95.2446517944336\n",
      "131 90.36111450195312\n",
      "132 85.73860168457031\n",
      "133 81.36449432373047\n",
      "134 77.2239761352539\n",
      "135 73.30635833740234\n",
      "136 69.5998764038086\n",
      "137 66.08796691894531\n",
      "138 62.76194763183594\n",
      "139 59.6099967956543\n",
      "140 56.62461853027344\n",
      "141 53.79463577270508\n",
      "142 51.1120491027832\n",
      "143 48.570037841796875\n",
      "144 46.16010665893555\n",
      "145 43.8751106262207\n",
      "146 41.70851135253906\n",
      "147 39.652164459228516\n",
      "148 37.70201110839844\n",
      "149 35.85185623168945\n",
      "150 34.09566879272461\n",
      "151 32.430240631103516\n",
      "152 30.848081588745117\n",
      "153 29.346847534179688\n",
      "154 27.921525955200195\n",
      "155 26.568065643310547\n",
      "156 25.282859802246094\n",
      "157 24.062353134155273\n",
      "158 22.902721405029297\n",
      "159 21.80103302001953\n",
      "160 20.75445556640625\n",
      "161 19.76023292541504\n",
      "162 18.81538963317871\n",
      "163 17.91699981689453\n",
      "164 17.063562393188477\n",
      "165 16.251447677612305\n",
      "166 15.480478286743164\n",
      "167 14.7461576461792\n",
      "168 14.048853874206543\n",
      "169 13.384921073913574\n",
      "170 12.753232955932617\n",
      "171 12.15329360961914\n",
      "172 11.581561088562012\n",
      "173 11.037906646728516\n",
      "174 10.520452499389648\n",
      "175 10.028068542480469\n",
      "176 9.559532165527344\n",
      "177 9.113683700561523\n",
      "178 8.689239501953125\n",
      "179 8.285364151000977\n",
      "180 7.900692939758301\n",
      "181 7.534212112426758\n",
      "182 7.185410022735596\n",
      "183 6.853180885314941\n",
      "184 6.536753177642822\n",
      "185 6.235112190246582\n",
      "186 5.9483642578125\n",
      "187 5.674650192260742\n",
      "188 5.414318561553955\n",
      "189 5.165822982788086\n",
      "190 4.929381370544434\n",
      "191 4.703871726989746\n",
      "192 4.488953113555908\n",
      "193 4.284547328948975\n",
      "194 4.089004039764404\n",
      "195 3.903078317642212\n",
      "196 3.7258729934692383\n",
      "197 3.556763172149658\n",
      "198 3.3954906463623047\n",
      "199 3.241642713546753\n",
      "200 3.095125675201416\n",
      "201 2.9551334381103516\n",
      "202 2.82200026512146\n",
      "203 2.6948862075805664\n",
      "204 2.5735902786254883\n",
      "205 2.4576449394226074\n",
      "206 2.3473503589630127\n",
      "207 2.241990566253662\n",
      "208 2.1414706707000732\n",
      "209 2.045656204223633\n",
      "210 1.9540730714797974\n",
      "211 1.866957426071167\n",
      "212 1.7835224866867065\n",
      "213 1.7040561437606812\n",
      "214 1.628238320350647\n",
      "215 1.5558288097381592\n",
      "216 1.4865351915359497\n",
      "217 1.420632004737854\n",
      "218 1.3574548959732056\n",
      "219 1.2973111867904663\n",
      "220 1.2399303913116455\n",
      "221 1.1850935220718384\n",
      "222 1.1326111555099487\n",
      "223 1.0826232433319092\n",
      "224 1.03482985496521\n",
      "225 0.9892461895942688\n",
      "226 0.9457422494888306\n",
      "227 0.9041483402252197\n",
      "228 0.8642803430557251\n",
      "229 0.8263261318206787\n",
      "230 0.7901325225830078\n",
      "231 0.7554193139076233\n",
      "232 0.7222601175308228\n",
      "233 0.6906619071960449\n",
      "234 0.660499095916748\n",
      "235 0.6316564083099365\n",
      "236 0.6040122509002686\n",
      "237 0.5776293873786926\n",
      "238 0.5524633526802063\n",
      "239 0.5283718705177307\n",
      "240 0.5054064393043518\n",
      "241 0.48336875438690186\n",
      "242 0.462360143661499\n",
      "243 0.4423529803752899\n",
      "244 0.4231348931789398\n",
      "245 0.40463462471961975\n",
      "246 0.38715121150016785\n",
      "247 0.3703729808330536\n",
      "248 0.3543800413608551\n",
      "249 0.33898305892944336\n",
      "250 0.3243408501148224\n",
      "251 0.3103303611278534\n",
      "252 0.2969290316104889\n",
      "253 0.2841150760650635\n",
      "254 0.27188655734062195\n",
      "255 0.2601483464241028\n",
      "256 0.2488916963338852\n",
      "257 0.23818214237689972\n",
      "258 0.2279687523841858\n",
      "259 0.21819332242012024\n",
      "260 0.2087891846895218\n",
      "261 0.19988124072551727\n",
      "262 0.1913154423236847\n",
      "263 0.18309825658798218\n",
      "264 0.17523042857646942\n",
      "265 0.1677253693342209\n",
      "266 0.16055940091609955\n",
      "267 0.15368911623954773\n",
      "268 0.14711838960647583\n",
      "269 0.1407998651266098\n",
      "270 0.13481251895427704\n",
      "271 0.12904341518878937\n",
      "272 0.12357582896947861\n",
      "273 0.1182810515165329\n",
      "274 0.11320727318525314\n",
      "275 0.10838961601257324\n",
      "276 0.10378214716911316\n",
      "277 0.09933417290449142\n",
      "278 0.09512142837047577\n",
      "279 0.0910932645201683\n",
      "280 0.08723676204681396\n",
      "281 0.08352027088403702\n",
      "282 0.0799781009554863\n",
      "283 0.07660028338432312\n",
      "284 0.07334762066602707\n",
      "285 0.07023528963327408\n",
      "286 0.06728467345237732\n",
      "287 0.06440258026123047\n",
      "288 0.06169101223349571\n",
      "289 0.05910221487283707\n",
      "290 0.056589819490909576\n",
      "291 0.05419410020112991\n",
      "292 0.05191570147871971\n",
      "293 0.04974040389060974\n",
      "294 0.04764079302549362\n",
      "295 0.04562182351946831\n",
      "296 0.04369659721851349\n",
      "297 0.04187086969614029\n",
      "298 0.04011410474777222\n",
      "299 0.03841429203748703\n",
      "300 0.03682076185941696\n",
      "301 0.035268377512693405\n",
      "302 0.03378187119960785\n",
      "303 0.03236502781510353\n",
      "304 0.031010083854198456\n",
      "305 0.0297164898365736\n",
      "306 0.028483852744102478\n",
      "307 0.02727801911532879\n",
      "308 0.02613838016986847\n",
      "309 0.025061270222067833\n",
      "310 0.024011867120862007\n",
      "311 0.023014504462480545\n",
      "312 0.022067558020353317\n",
      "313 0.021140752360224724\n",
      "314 0.020270686596632004\n",
      "315 0.019425252452492714\n",
      "316 0.018610160797834396\n",
      "317 0.017841923981904984\n",
      "318 0.017116891220211983\n",
      "319 0.016397790983319283\n",
      "320 0.01572156324982643\n",
      "321 0.01507548987865448\n",
      "322 0.01444558147341013\n",
      "323 0.013861590065062046\n",
      "324 0.01329110935330391\n",
      "325 0.012742538005113602\n",
      "326 0.012223141267895699\n",
      "327 0.011714905500411987\n",
      "328 0.01124963816255331\n",
      "329 0.010799283161759377\n",
      "330 0.010352670215070248\n",
      "331 0.009937437251210213\n",
      "332 0.009530343115329742\n",
      "333 0.009155265986919403\n",
      "334 0.00877799466252327\n",
      "335 0.008423900231719017\n",
      "336 0.008090285584330559\n",
      "337 0.007765153888612986\n",
      "338 0.007457123603671789\n",
      "339 0.007156534120440483\n",
      "340 0.006876325234770775\n",
      "341 0.006608116906136274\n",
      "342 0.006343372631818056\n",
      "343 0.006092383526265621\n",
      "344 0.00585734099149704\n",
      "345 0.005626716185361147\n",
      "346 0.0054033128544688225\n",
      "347 0.005196417216211557\n",
      "348 0.004992680158466101\n",
      "349 0.0047962418757379055\n",
      "350 0.004611120093613863\n",
      "351 0.004433713853359222\n",
      "352 0.004273587372153997\n",
      "353 0.00411219010129571\n",
      "354 0.0039549958892166615\n",
      "355 0.003805068787187338\n",
      "356 0.003664392279461026\n",
      "357 0.0035259707365185022\n",
      "358 0.0033916113898158073\n",
      "359 0.0032656353432685137\n",
      "360 0.003143234644085169\n",
      "361 0.0030283143278211355\n",
      "362 0.0029139076359570026\n",
      "363 0.00280824420042336\n",
      "364 0.002707664156332612\n",
      "365 0.0026073805056512356\n",
      "366 0.0025132973678410053\n",
      "367 0.002423018915578723\n",
      "368 0.002334359334781766\n",
      "369 0.0022510397247970104\n",
      "370 0.0021736754570156336\n",
      "371 0.002097667660564184\n",
      "372 0.002021204913035035\n",
      "373 0.001951374113559723\n",
      "374 0.0018818376120179892\n",
      "375 0.0018186001107096672\n",
      "376 0.001752997632138431\n",
      "377 0.0016965960385277867\n",
      "378 0.0016387200448662043\n",
      "379 0.0015849815681576729\n",
      "380 0.0015342984115704894\n",
      "381 0.0014818392228335142\n",
      "382 0.0014348989352583885\n",
      "383 0.001387501135468483\n",
      "384 0.0013427695957943797\n",
      "385 0.0013006089720875025\n",
      "386 0.0012591169215738773\n",
      "387 0.0012180623598396778\n",
      "388 0.0011794536840170622\n",
      "389 0.0011427917052060366\n",
      "390 0.0011073738569393754\n",
      "391 0.0010731088696047664\n",
      "392 0.0010393343400210142\n",
      "393 0.0010077538900077343\n",
      "394 0.0009754151105880737\n",
      "395 0.0009472119272686541\n",
      "396 0.0009187785326503217\n",
      "397 0.000892230193130672\n",
      "398 0.0008661278407089412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "399 0.0008407257264479995\n",
      "400 0.000815213134046644\n",
      "401 0.0007915767491795123\n",
      "402 0.0007707122713327408\n",
      "403 0.0007489375420846045\n",
      "404 0.000727669452317059\n",
      "405 0.0007082887459546328\n",
      "406 0.0006876115803606808\n",
      "407 0.0006683834944851696\n",
      "408 0.0006496653077192605\n",
      "409 0.0006329094176180661\n",
      "410 0.0006157293100841343\n",
      "411 0.0005990762729197741\n",
      "412 0.0005814794567413628\n",
      "413 0.000567686278373003\n",
      "414 0.0005518735270015895\n",
      "415 0.0005379949579946697\n",
      "416 0.0005239408928900957\n",
      "417 0.0005109445773996413\n",
      "418 0.0004979264922440052\n",
      "419 0.00048516166862100363\n",
      "420 0.00047292609815485775\n",
      "421 0.00046157435281202197\n",
      "422 0.00044996861834079027\n",
      "423 0.0004384240019135177\n",
      "424 0.00042779595241881907\n",
      "425 0.00041692424565553665\n",
      "426 0.00040680382517166436\n",
      "427 0.0003973639686591923\n",
      "428 0.00038905185647308826\n",
      "429 0.0003787998575717211\n",
      "430 0.0003707826544996351\n",
      "431 0.00036169192753732204\n",
      "432 0.000353094597812742\n",
      "433 0.00034433742985129356\n",
      "434 0.0003374666557647288\n",
      "435 0.00032953821937553585\n",
      "436 0.00032214936800301075\n",
      "437 0.0003153163706883788\n",
      "438 0.0003085153002757579\n",
      "439 0.0003025085316039622\n",
      "440 0.00029564049327746034\n",
      "441 0.00028953421860933304\n",
      "442 0.00028237374499440193\n",
      "443 0.0002765640674624592\n",
      "444 0.000271201366558671\n",
      "445 0.00026447424897924066\n",
      "446 0.0002593605313450098\n",
      "447 0.00025414914125576615\n",
      "448 0.0002492048079147935\n",
      "449 0.00024348462466150522\n",
      "450 0.00023877313651610166\n",
      "451 0.00023436557967215776\n",
      "452 0.00022974728199187666\n",
      "453 0.00022461952175945044\n",
      "454 0.0002207022625952959\n",
      "455 0.00021605983783956617\n",
      "456 0.00021187499805819243\n",
      "457 0.00020785650121979415\n",
      "458 0.00020345007942523807\n",
      "459 0.00019933993462473154\n",
      "460 0.00019611632160376757\n",
      "461 0.00019222548871766776\n",
      "462 0.00018828580505214632\n",
      "463 0.0001848039828473702\n",
      "464 0.00018153083510696888\n",
      "465 0.00017892553296405822\n",
      "466 0.000176344983628951\n",
      "467 0.00017299178580287844\n",
      "468 0.00016963323287200183\n",
      "469 0.00016608607256785035\n",
      "470 0.00016358387074433267\n",
      "471 0.00016125678666867316\n",
      "472 0.0001584107376402244\n",
      "473 0.00015556039579678327\n",
      "474 0.00015317053475882858\n",
      "475 0.00015026793698780239\n",
      "476 0.00014743275824002922\n",
      "477 0.0001448132679797709\n",
      "478 0.0001423822541255504\n",
      "479 0.00014035491039976478\n",
      "480 0.00013796021812595427\n",
      "481 0.00013552745804190636\n",
      "482 0.00013356348790694028\n",
      "483 0.0001311987725785002\n",
      "484 0.00012938720465172082\n",
      "485 0.00012731325114145875\n",
      "486 0.00012496201088652015\n",
      "487 0.00012300566595513374\n",
      "488 0.0001213427385664545\n",
      "489 0.00011976331006735563\n",
      "490 0.0001178622042061761\n",
      "491 0.00011649498628685251\n",
      "492 0.00011450723832240328\n",
      "493 0.00011308074317639694\n",
      "494 0.00011110831110272557\n",
      "495 0.0001094452163670212\n",
      "496 0.00010817937436513603\n",
      "497 0.00010636786464601755\n",
      "498 0.00010475497401785105\n",
      "499 0.00010369243682362139\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H, bias=False), # w_1 * x + b_1\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out, bias=False),\n",
    ")\n",
    "\n",
    "torch.nn.init.normal_(model[0].weight)\n",
    "torch.nn.init.normal_(model[2].weight)\n",
    "\n",
    "# model = model.cuda()\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "# learning_rate = 1e-4\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    print(it, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch: 自定义 nn Modules\n",
    "--------------------------\n",
    "\n",
    "我们可以定义一个模型，这个模型继承自nn.Module类。如果需要定义一个比Sequential模型更加复杂的模型，就需要定义nn.Module模型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 627.3970336914062\n",
      "1 611.1285400390625\n",
      "2 595.31689453125\n",
      "3 579.910400390625\n",
      "4 564.9271850585938\n",
      "5 550.3590087890625\n",
      "6 536.1961669921875\n",
      "7 522.4404296875\n",
      "8 509.03594970703125\n",
      "9 495.9280700683594\n",
      "10 483.1991271972656\n",
      "11 470.7916259765625\n",
      "12 458.69512939453125\n",
      "13 446.97943115234375\n",
      "14 435.7530822753906\n",
      "15 424.87615966796875\n",
      "16 414.29534912109375\n",
      "17 403.9889221191406\n",
      "18 393.9405822753906\n",
      "19 384.1108093261719\n",
      "20 374.5871887207031\n",
      "21 365.31689453125\n",
      "22 356.3175964355469\n",
      "23 347.5909118652344\n",
      "24 339.0564880371094\n",
      "25 330.7277526855469\n",
      "26 322.60174560546875\n",
      "27 314.66668701171875\n",
      "28 306.9889831542969\n",
      "29 299.5657043457031\n",
      "30 292.3930969238281\n",
      "31 285.4082336425781\n",
      "32 278.5841064453125\n",
      "33 271.9277648925781\n",
      "34 265.4345703125\n",
      "35 259.0935363769531\n",
      "36 252.884033203125\n",
      "37 246.8143310546875\n",
      "38 240.864013671875\n",
      "39 235.03262329101562\n",
      "40 229.34420776367188\n",
      "41 223.77505493164062\n",
      "42 218.32476806640625\n",
      "43 212.99063110351562\n",
      "44 207.78125\n",
      "45 202.6981201171875\n",
      "46 197.71563720703125\n",
      "47 192.83370971679688\n",
      "48 188.05160522460938\n",
      "49 183.3824462890625\n",
      "50 178.80023193359375\n",
      "51 174.30784606933594\n",
      "52 169.91656494140625\n",
      "53 165.6317901611328\n",
      "54 161.43165588378906\n",
      "55 157.33067321777344\n",
      "56 153.3175811767578\n",
      "57 149.37823486328125\n",
      "58 145.5091094970703\n",
      "59 141.72801208496094\n",
      "60 138.03196716308594\n",
      "61 134.42578125\n",
      "62 130.89434814453125\n",
      "63 127.43917083740234\n",
      "64 124.06126403808594\n",
      "65 120.7606201171875\n",
      "66 117.53297424316406\n",
      "67 114.37368774414062\n",
      "68 111.27806091308594\n",
      "69 108.24208068847656\n",
      "70 105.2750244140625\n",
      "71 102.37030029296875\n",
      "72 99.52387237548828\n",
      "73 96.73075103759766\n",
      "74 93.99861145019531\n",
      "75 91.32463073730469\n",
      "76 88.71073150634766\n",
      "77 86.1602554321289\n",
      "78 83.66371154785156\n",
      "79 81.2225341796875\n",
      "80 78.83152770996094\n",
      "81 76.49999237060547\n",
      "82 74.22273254394531\n",
      "83 71.99769592285156\n",
      "84 69.82504272460938\n",
      "85 67.70761108398438\n",
      "86 65.64385986328125\n",
      "87 63.63422393798828\n",
      "88 61.66884994506836\n",
      "89 59.7559700012207\n",
      "90 57.89274215698242\n",
      "91 56.07460403442383\n",
      "92 54.301902770996094\n",
      "93 52.570926666259766\n",
      "94 50.888214111328125\n",
      "95 49.24935531616211\n",
      "96 47.652034759521484\n",
      "97 46.097530364990234\n",
      "98 44.58283615112305\n",
      "99 43.10759735107422\n",
      "100 41.67060470581055\n",
      "101 40.271507263183594\n",
      "102 38.91142272949219\n",
      "103 37.586761474609375\n",
      "104 36.297725677490234\n",
      "105 35.04634094238281\n",
      "106 33.833248138427734\n",
      "107 32.65315628051758\n",
      "108 31.50496482849121\n",
      "109 30.39141845703125\n",
      "110 29.31031608581543\n",
      "111 28.26127815246582\n",
      "112 27.24369239807129\n",
      "113 26.25568199157715\n",
      "114 25.298627853393555\n",
      "115 24.368305206298828\n",
      "116 23.465190887451172\n",
      "117 22.590538024902344\n",
      "118 21.742780685424805\n",
      "119 20.92242431640625\n",
      "120 20.126232147216797\n",
      "121 19.35456085205078\n",
      "122 18.607746124267578\n",
      "123 17.88462257385254\n",
      "124 17.185882568359375\n",
      "125 16.510055541992188\n",
      "126 15.856345176696777\n",
      "127 15.224493026733398\n",
      "128 14.61458683013916\n",
      "129 14.025089263916016\n",
      "130 13.456539154052734\n",
      "131 12.908470153808594\n",
      "132 12.379345893859863\n",
      "133 11.868867874145508\n",
      "134 11.377185821533203\n",
      "135 10.903217315673828\n",
      "136 10.446348190307617\n",
      "137 10.00663948059082\n",
      "138 9.58306884765625\n",
      "139 9.174960136413574\n",
      "140 8.782583236694336\n",
      "141 8.405747413635254\n",
      "142 8.043668746948242\n",
      "143 7.695374011993408\n",
      "144 7.361196994781494\n",
      "145 7.039714336395264\n",
      "146 6.730838775634766\n",
      "147 6.434250354766846\n",
      "148 6.1493611335754395\n",
      "149 5.876176357269287\n",
      "150 5.614273548126221\n",
      "151 5.362517356872559\n",
      "152 5.121194839477539\n",
      "153 4.889812469482422\n",
      "154 4.6678643226623535\n",
      "155 4.455075740814209\n",
      "156 4.2512946128845215\n",
      "157 4.056135654449463\n",
      "158 3.869044542312622\n",
      "159 3.6898880004882812\n",
      "160 3.5182390213012695\n",
      "161 3.353853940963745\n",
      "162 3.196502447128296\n",
      "163 3.045853853225708\n",
      "164 2.9018678665161133\n",
      "165 2.7639174461364746\n",
      "166 2.632017135620117\n",
      "167 2.5059618949890137\n",
      "168 2.3854684829711914\n",
      "169 2.2704145908355713\n",
      "170 2.1606667041778564\n",
      "171 2.055724859237671\n",
      "172 1.9555984735488892\n",
      "173 1.860053539276123\n",
      "174 1.768800139427185\n",
      "175 1.6817657947540283\n",
      "176 1.5987169742584229\n",
      "177 1.5194449424743652\n",
      "178 1.4437516927719116\n",
      "179 1.3714617490768433\n",
      "180 1.3025473356246948\n",
      "181 1.236851453781128\n",
      "182 1.1741780042648315\n",
      "183 1.1145415306091309\n",
      "184 1.0576282739639282\n",
      "185 1.0034757852554321\n",
      "186 0.9518811702728271\n",
      "187 0.9027747511863708\n",
      "188 0.8560277819633484\n",
      "189 0.8115973472595215\n",
      "190 0.7692156434059143\n",
      "191 0.7289477586746216\n",
      "192 0.6906320452690125\n",
      "193 0.6542015671730042\n",
      "194 0.6195531487464905\n",
      "195 0.5866392254829407\n",
      "196 0.5553579926490784\n",
      "197 0.5256213545799255\n",
      "198 0.49740004539489746\n",
      "199 0.47058412432670593\n",
      "200 0.4451182782649994\n",
      "201 0.42096826434135437\n",
      "202 0.39804598689079285\n",
      "203 0.37631842494010925\n",
      "204 0.35572418570518494\n",
      "205 0.33617129921913147\n",
      "206 0.3176569938659668\n",
      "207 0.30010488629341125\n",
      "208 0.28346678614616394\n",
      "209 0.2677038013935089\n",
      "210 0.2527713477611542\n",
      "211 0.23863986134529114\n",
      "212 0.22524841129779816\n",
      "213 0.21256324648857117\n",
      "214 0.20057189464569092\n",
      "215 0.18921174108982086\n",
      "216 0.17847566306591034\n",
      "217 0.16831322014331818\n",
      "218 0.15870273113250732\n",
      "219 0.1496158242225647\n",
      "220 0.14102934300899506\n",
      "221 0.1329077035188675\n",
      "222 0.12524552643299103\n",
      "223 0.11800146102905273\n",
      "224 0.11115183681249619\n",
      "225 0.10469198226928711\n",
      "226 0.09858433157205582\n",
      "227 0.09282246232032776\n",
      "228 0.08738164603710175\n",
      "229 0.08224461227655411\n",
      "230 0.0773981437087059\n",
      "231 0.07282596826553345\n",
      "232 0.06851378083229065\n",
      "233 0.06444767862558365\n",
      "234 0.06061762943863869\n",
      "235 0.05700676515698433\n",
      "236 0.05360185727477074\n",
      "237 0.05039516091346741\n",
      "238 0.04737427830696106\n",
      "239 0.04452678561210632\n",
      "240 0.041845954954624176\n",
      "241 0.039320409297943115\n",
      "242 0.03694316744804382\n",
      "243 0.03470401093363762\n",
      "244 0.03259769082069397\n",
      "245 0.030615782365202904\n",
      "246 0.02875065989792347\n",
      "247 0.026996508240699768\n",
      "248 0.025345856323838234\n",
      "249 0.023793956264853477\n",
      "250 0.02233453467488289\n",
      "251 0.02096247300505638\n",
      "252 0.019673069939017296\n",
      "253 0.018460463732481003\n",
      "254 0.01732272282242775\n",
      "255 0.01625286042690277\n",
      "256 0.015247181057929993\n",
      "257 0.014302569441497326\n",
      "258 0.013415406458079815\n",
      "259 0.012582738883793354\n",
      "260 0.011801682412624359\n",
      "261 0.01106759998947382\n",
      "262 0.010378570295870304\n",
      "263 0.009732350707054138\n",
      "264 0.009125886484980583\n",
      "265 0.008556630462408066\n",
      "266 0.00802250113338232\n",
      "267 0.007521464955061674\n",
      "268 0.00705182459205389\n",
      "269 0.006611056160181761\n",
      "270 0.006197912152856588\n",
      "271 0.00581024307757616\n",
      "272 0.005446880590170622\n",
      "273 0.005106091033667326\n",
      "274 0.0047867754474282265\n",
      "275 0.004487427417188883\n",
      "276 0.004206698853522539\n",
      "277 0.003943657968193293\n",
      "278 0.003697344334796071\n",
      "279 0.0034662941470742226\n",
      "280 0.0032497523352503777\n",
      "281 0.0030469007324427366\n",
      "282 0.00285677844658494\n",
      "283 0.0026786820963025093\n",
      "284 0.0025118438061326742\n",
      "285 0.002355442848056555\n",
      "286 0.0022089011035859585\n",
      "287 0.002071609254926443\n",
      "288 0.001942999311722815\n",
      "289 0.0018225617241114378\n",
      "290 0.0017095815856009722\n",
      "291 0.0016037484165281057\n",
      "292 0.001504580955952406\n",
      "293 0.0014116399688646197\n",
      "294 0.0013245870359241962\n",
      "295 0.0012430025963112712\n",
      "296 0.0011665726779028773\n",
      "297 0.0010949105489999056\n",
      "298 0.0010278031695634127\n",
      "299 0.0009648564737290144\n",
      "300 0.0009058618452399969\n",
      "301 0.000850573880597949\n",
      "302 0.0007987353601492941\n",
      "303 0.0007501696236431599\n",
      "304 0.0007046507089398801\n",
      "305 0.0006619286141358316\n",
      "306 0.0006218631169758737\n",
      "307 0.0005842969403602183\n",
      "308 0.0005490569164976478\n",
      "309 0.0005160124273970723\n",
      "310 0.0004850311088375747\n",
      "311 0.0004559616791084409\n",
      "312 0.00042868213495239615\n",
      "313 0.0004030781565234065\n",
      "314 0.0003790629270952195\n",
      "315 0.00035653699887916446\n",
      "316 0.0003353612555656582\n",
      "317 0.00031549588311463594\n",
      "318 0.00029683884349651635\n",
      "319 0.0002793225285131484\n",
      "320 0.0002628726651892066\n",
      "321 0.0002474188804626465\n",
      "322 0.00023289950331673026\n",
      "323 0.0002192624961026013\n",
      "324 0.0002064516011159867\n",
      "325 0.0001944107934832573\n",
      "326 0.00018308463040739298\n",
      "327 0.00017244800983462483\n",
      "328 0.0001624445285415277\n",
      "329 0.00015303811233025044\n",
      "330 0.00014419431681744754\n",
      "331 0.00013587491412181407\n",
      "332 0.0001280505966860801\n",
      "333 0.00012068462820025161\n",
      "334 0.00011375809117453173\n",
      "335 0.0001072403319994919\n",
      "336 0.0001011048152577132\n",
      "337 9.532864351058379e-05\n",
      "338 8.988985064206645e-05\n",
      "339 8.476998482365161e-05\n",
      "340 7.99519766587764e-05\n",
      "341 7.541357626905665e-05\n",
      "342 7.113078754628077e-05\n",
      "343 6.71053712721914e-05\n",
      "344 6.330767791951075e-05\n",
      "345 5.973035149509087e-05\n",
      "346 5.63616631552577e-05\n",
      "347 5.318355033523403e-05\n",
      "348 5.018731826567091e-05\n",
      "349 4.736364644486457e-05\n",
      "350 4.470092972042039e-05\n",
      "351 4.219064430799335e-05\n",
      "352 3.982314592576586e-05\n",
      "353 3.759191167773679e-05\n",
      "354 3.548479435266927e-05\n",
      "355 3.349972394062206e-05\n",
      "356 3.162557914038189e-05\n",
      "357 2.985668834298849e-05\n",
      "358 2.818833672790788e-05\n",
      "359 2.66152110270923e-05\n",
      "360 2.5129138521151617e-05\n",
      "361 2.3725433493382297e-05\n",
      "362 2.2403080947697163e-05\n",
      "363 2.1153315174160525e-05\n",
      "364 1.997458457481116e-05\n",
      "365 1.886065183498431e-05\n",
      "366 1.7809301425586455e-05\n",
      "367 1.681631874816958e-05\n",
      "368 1.5879479178693146e-05\n",
      "369 1.4994064258644357e-05\n",
      "370 1.4158721569401678e-05\n",
      "371 1.336951845587464e-05\n",
      "372 1.2625248928088695e-05\n",
      "373 1.1920925317099318e-05\n",
      "374 1.1255981007707305e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375 1.0628429663483985e-05\n",
      "376 1.003461511572823e-05\n",
      "377 9.473998034081887e-06\n",
      "378 8.944818546297029e-06\n",
      "379 8.445858838967979e-06\n",
      "380 7.97367738414323e-06\n",
      "381 7.5275370363669936e-06\n",
      "382 7.106205430318369e-06\n",
      "383 6.70767440169584e-06\n",
      "384 6.332363682304276e-06\n",
      "385 5.976620741421357e-06\n",
      "386 5.641676125378581e-06\n",
      "387 5.324543508322677e-06\n",
      "388 5.025209702580469e-06\n",
      "389 4.7420826376765035e-06\n",
      "390 4.474781690078089e-06\n",
      "391 4.222859388391953e-06\n",
      "392 3.984295290138107e-06\n",
      "393 3.7593920296785655e-06\n",
      "394 3.5475932236295193e-06\n",
      "395 3.3458875350333983e-06\n",
      "396 3.155816784783383e-06\n",
      "397 2.9769507818855345e-06\n",
      "398 2.808133103826549e-06\n",
      "399 2.6484437967155827e-06\n",
      "400 2.497099103493383e-06\n",
      "401 2.3547511318611214e-06\n",
      "402 2.220245960415923e-06\n",
      "403 2.093348030030029e-06\n",
      "404 1.9735789464903064e-06\n",
      "405 1.8605572904561996e-06\n",
      "406 1.7538062593303039e-06\n",
      "407 1.6528582591490704e-06\n",
      "408 1.557692485221196e-06\n",
      "409 1.4678712432214525e-06\n",
      "410 1.3830973557560355e-06\n",
      "411 1.3032359902354074e-06\n",
      "412 1.2278317171876552e-06\n",
      "413 1.1563047337403987e-06\n",
      "414 1.0887147254834417e-06\n",
      "415 1.0257170970362495e-06\n",
      "416 9.657691180109396e-07\n",
      "417 9.092781283470686e-07\n",
      "418 8.561765980630298e-07\n",
      "419 8.061544463089376e-07\n",
      "420 7.588564585603308e-07\n",
      "421 7.140693583096436e-07\n",
      "422 6.721536465192912e-07\n",
      "423 6.32282251444849e-07\n",
      "424 5.949363526269735e-07\n",
      "425 5.597185008809902e-07\n",
      "426 5.267506253403553e-07\n",
      "427 4.952068479724403e-07\n",
      "428 4.6582403001593775e-07\n",
      "429 4.379368192530819e-07\n",
      "430 4.1186419252881024e-07\n",
      "431 3.873043112889718e-07\n",
      "432 3.6391523394740943e-07\n",
      "433 3.421091605559923e-07\n",
      "434 3.214695141195989e-07\n",
      "435 3.021250165602396e-07\n",
      "436 2.83847754189992e-07\n",
      "437 2.6668087116377137e-07\n",
      "438 2.50543024549188e-07\n",
      "439 2.353662722498484e-07\n",
      "440 2.209195599789382e-07\n",
      "441 2.0757664742632187e-07\n",
      "442 1.9479750790196704e-07\n",
      "443 1.8288157832557772e-07\n",
      "444 1.7173709920825786e-07\n",
      "445 1.611578994697993e-07\n",
      "446 1.5124622620987793e-07\n",
      "447 1.418690089849406e-07\n",
      "448 1.3313426450167754e-07\n",
      "449 1.2488136746924283e-07\n",
      "450 1.1714068648416287e-07\n",
      "451 1.0988256349264702e-07\n",
      "452 1.0298971631073073e-07\n",
      "453 9.664092459615858e-08\n",
      "454 9.06481574247664e-08\n",
      "455 8.494245662404865e-08\n",
      "456 7.959040715377341e-08\n",
      "457 7.457788342435379e-08\n",
      "458 6.985533218539786e-08\n",
      "459 6.548977182774252e-08\n",
      "460 6.129572227564495e-08\n",
      "461 5.739152086903232e-08\n",
      "462 5.3807010402806554e-08\n",
      "463 5.038752703967475e-08\n",
      "464 4.717186641300941e-08\n",
      "465 4.420550325789918e-08\n",
      "466 4.1399751182780165e-08\n",
      "467 3.871957332535203e-08\n",
      "468 3.6227092437002284e-08\n",
      "469 3.393017777852947e-08\n",
      "470 3.1771730135687903e-08\n",
      "471 2.9731561212997804e-08\n",
      "472 2.7784357925497716e-08\n",
      "473 2.6015586129801704e-08\n",
      "474 2.4305983004069276e-08\n",
      "475 2.276066624062878e-08\n",
      "476 2.1300227359688506e-08\n",
      "477 1.9925753491634168e-08\n",
      "478 1.8622863251493982e-08\n",
      "479 1.740671073946487e-08\n",
      "480 1.6258802304491837e-08\n",
      "481 1.518746728379483e-08\n",
      "482 1.4220646882279198e-08\n",
      "483 1.3303387724761251e-08\n",
      "484 1.2420358075360127e-08\n",
      "485 1.1584798009778297e-08\n",
      "486 1.0829329433192925e-08\n",
      "487 1.0109831194426988e-08\n",
      "488 9.46346645491758e-09\n",
      "489 8.84216166952001e-09\n",
      "490 8.242074578390657e-09\n",
      "491 7.699227033697298e-09\n",
      "492 7.207746843818086e-09\n",
      "493 6.740380253944522e-09\n",
      "494 6.295481469464903e-09\n",
      "495 5.875820718870273e-09\n",
      "496 5.496554322803604e-09\n",
      "497 5.145230907999121e-09\n",
      "498 4.8133599328537e-09\n",
      "499 4.492759053675854e-09\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 随机创建一些训练数据\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        # define the model architecture\n",
    "        # 线性层中的D_in, H, D_out都为参数W的维度，初始化中不涉及输入X和Y\n",
    "        self.linear1 = torch.nn.Linear(D_in, H, bias=False)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear2(self.linear1(x).clamp(min=0))\n",
    "        return y_pred\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "loss_fn = nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "'''\n",
    "训练时，三句话的顺序为\n",
    "1. optimizer.zero_grad()\n",
    "2. loss.backward()\n",
    "3. optimizer.step()\n",
    "'''\n",
    "for it in range(500):\n",
    "    # Forward pass\n",
    "    y_pred = model(x) # model.forward() \n",
    "    \n",
    "    # compute loss\n",
    "    loss = loss_fn(y_pred, y) # computation graph\n",
    "    print(it, loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update model parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小结\n",
    "\n",
    "- 定义模型时，一般分为：初始化和前向传播。深度学习的框架大致为：定义输入输出数据、定义模型、定义损失函数和优化方法、训练模型、调参。\n",
    "- 模型性能不好时，可以调节学习率、更换优化方法、初始化参数\n",
    "- Adam的lr一般为1e-3——1e-4，SGD的lr一般为1e-6——1e-7\n",
    "- 定义模型时，初始化中不涉及输入X和Y，均为参数的维度\n",
    "- 训练时，三句话的顺序为\n",
    "    1. optimizer.zero_grad()\n",
    "    2. loss.backward()\n",
    "    3. optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
